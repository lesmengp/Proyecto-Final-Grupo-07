{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "\n",
    "import json\n",
    "import os                       # Libreria para manejo de arhivos y directorios del sistema\n",
    "import glob                     # crear listas de archivos a partir de b√∫squedas con comodines en un directorio\n",
    "import pyarrow as pa\n",
    "import jsonlines                # Permite procesar un registro por vez\n",
    "import pyarrow.parquet as pq    # Permite leer archivos en formato parquet\n",
    "import pickle5 as pickle        # Permite leer archivos con formatp pkl, para archivos version 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion que permite buscar archivos en un directorio segun un filtro deseado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recibe el directorio a trabajar y el patron o filtro que se desea mostrar.\n",
    "\n",
    "def listar_archivos_segun_patron_con_directorio(directorio, patron):\n",
    "    return glob.glob(directorio + patron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "`Datasets: checkin - Cargando los archivos parquet reducidos como un Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/checkin/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"checkin-reducido_*.parquet\")\n",
    "df_list = []                                        # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                          # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_checkin = pq.read_table(source=archivo)   # Se crea una tabla con cada archivo\n",
    "    df = tabla_checkin.to_pandas()                  # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                              # Se adicionan a los dataframe\n",
    "df_checkin_reducido_concatenado = pd.concat(df_list)# Se concatenan todos los DataFrames en uno solo\n",
    "df_checkin_reducido_concatenado = df_checkin_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "`Datasets: tip - Cargando los archivos parquet reducidos como un Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/tip/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"tip-reducido*.parquet\")\n",
    "df_list = []                                        # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                          # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_review = pq.read_table(source=archivo)    # Se crea una tabla con cada archivo\n",
    "    df = tabla_review.to_pandas()                   # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                              # Se adicionan a los dataframe\n",
    "df_tip_reducido_concatenado = pd.concat(df_list) # Se concatenan todos los DataFrames en uno solo\n",
    "df_tip_reducido_concatenado = df_tip_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "`Datasets: review - Cargando los archivos parquet reducidos como un Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/review/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"review-reducido_*.parquet\")\n",
    "df_list = []                                        # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                          # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_review = pq.read_table(source=archivo)    # Se crea una tabla con cada archivo\n",
    "    df = tabla_review.to_pandas()                   # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                              # Se adicionan a los dataframe\n",
    "df_review_reducido_concatenado = pd.concat(df_list) # Se concatenan todos los DataFrames en uno solo\n",
    "df_review_reducido_concatenado = df_review_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "`Datasets: user - Cargando los archivos parquet reducidos como un Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio = \"../Datasets/Datasets_Yelp/user/\"\n",
    "ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"user-reducido_*.parquet\")\n",
    "df_list = []                                    # Un dataframe vacio para ir adicionando\n",
    "for archivo in ls_archivo:                      # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "    tabla_user = pq.read_table(source=archivo)  # Se crea una tabla con cada archivo\n",
    "    df = tabla_user.to_pandas()                 # Se transforma la tabla en un dataframe\n",
    "    df_list.append(df)                          # Se adicionan a los dataframe\n",
    "df_user_reducido_concatenado = pd.concat(df_list)                                   # Se concatenan todos los DataFrames en uno solo\n",
    "df_user_reducido_concatenado = df_user_reducido_concatenado.reset_index(drop=True)  # Se resetea el indice a uno secuencial y consecutivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "`Datasets: metadata-sitios - Cargando los archivos parquet reducidos como un Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo = \"../Datasets/Datasets_Google_Maps/metadata-sitios/metadata-sitios_reducido.parquet\"\n",
    "tabla_sitios = pq.read_table(source=archivo)                      # Se crea una tabla con cada archivo\n",
    "df_sitios = tabla_sitios.to_pandas()                              # Se transforma la tabla en un dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "`Datasets: metadata-estados - Cargando los archivos parquet reducidos como un Dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directorio_origen = \"../Datasets/Datasets_Google_Maps/reviews-estados/\"    # Debido a la cantidad de directorios y archivos se permitira tener separados los archivos de origen.\n",
    "directorios_reviews_estados = os.listdir(directorio_origen)                # Se obtiene un listado de todos los subdirectorios del directorio origen.\n",
    "for sub_dir in directorios_reviews_estados:                                # Ciclo por cada subdirectorio\n",
    "    nombre_archivo = sub_dir.replace(\"review-\", '')\n",
    "    archivos_reviews_estados = os.listdir(directorio_origen + sub_dir)     # Se guarda el directorio y el subdirectorio a utilizar\n",
    "    directorio = directorio_origen + sub_dir + \"/\"\n",
    "    ls_archivo = listar_archivos_segun_patron_con_directorio(directorio, \"*.parquet\")\n",
    "    df_list = []                                                            # Un dataframe vacio para ir adicionando\n",
    "    for archivo in ls_archivo:                                              # Leer cada archivo parquet en un DataFrame de Pandas y adicionarlo\n",
    "        # print(directorio, nombre_archivo)\n",
    "        # print(ls_archivo)\n",
    "        tabla_estados = pq.read_table(source=archivo)                       # Se crea una tabla con cada archivo\n",
    "        df = tabla_estados.to_pandas()                                      # Se transforma la tabla en un dataframe\n",
    "        # df['estado'] = nombre_archivo\n",
    "        df_list.append(df)                                                  # Se adicionan a los dataframe\n",
    "    df_estados_concatenado = pd.concat(df_list)                             # Se concatenan todos los DataFrames en uno solo\n",
    "    df['estado'] = nombre_archivo\n",
    "df_estados_concatenado = df_estados_concatenado.reset_index(drop=True)      # Se resetea el indice a uno secuencial y consecutivo        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
